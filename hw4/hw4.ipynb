{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules for data\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# modules for metrics\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "# modules for building model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Load data and split into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data to pandas\n",
    "dui = pd.read_csv('data/dui.csv')\n",
    "\n",
    "# encode the gender\n",
    "gender = dui.iloc[:,1]\n",
    "le = LabelEncoder()\n",
    "le.fit(gender)\n",
    "encoded_column = le.transform(gender)\n",
    "dui['Gender'] = encoded_column\n",
    "\n",
    "# convert the fatality for binary classification\n",
    "dui['Fatality'].where(dui['Fatality']<=0, 1, True)\n",
    "\n",
    "# split the variables and labels\n",
    "X = dui.iloc[:,:-1]\n",
    "y = dui.iloc[:,-1]\n",
    "\n",
    "# split the data in to train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dui[dui['Fatality']==-1]['Fatality']=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **The data would be split into 80% of train dataset and 20% of test dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Model and Hyperparameter Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Since the dependent variable is not continuous, I think linear regression would previde higher accuracy, so I choose to use Logistic regressio instead.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5628\n",
      "2690\n"
     ]
    }
   ],
   "source": [
    "print(len(dui[dui['Fatality']==0]))\n",
    "print(len(dui[dui['Fatality']==1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The number of accident that people not die in dataset is 2 times bigger than the number of accident that people die, So for the logistic regression model, the weight of the class are set to 0.68 : 0.32**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the logistic regression model\n",
    "lr = LogisticRegression(class_weight={0:0.68, 1:0.32})\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# predicting using test dataset\n",
    "predictions = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8533653846153846\n",
      "F1-Score:  0.7925170068027212\n",
      "Confusion matrix:\n",
      "      0    1\n",
      "0  954  165\n",
      "1   79  466\n"
     ]
    }
   ],
   "source": [
    "# metrics\n",
    "print('Accuracy: ', lr.score(X_test, y_test))\n",
    "print('F1-Score: ', f1_score(y_test, predictions))\n",
    "print('Confusion matrix:\\n', pd.DataFrame(confusion_matrix(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Since need to prevent the overfitting problem, finding the best max depth of decision tree is necessary. So I would do a 10-folds grid search cross validation on parameter citerion, max_depth and class_weight**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise-deprecating',\n",
       "       estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'criterion': ('entropy', 'gini'), 'max_depth': [2, 3, 4, 5, 6], 'class_weight': ({0: 0.68, 1: 0.32}, 'balanced', None)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grid search cross validation on parameters \"criterion\" and \"max_depth\"\n",
    "dt = DecisionTreeClassifier()\n",
    "folds = 10\n",
    "grid_params = {'criterion':('entropy', 'gini'), 'max_depth':[2,3,4,5,6], 'class_weight':({0:0.68, 1:0.32}, 'balanced', None)}\n",
    "classifier = GridSearchCV(dt, grid_params, cv=folds)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score:  0.8969196093163035 \n",
      "Best Tree:  DecisionTreeClassifier(class_weight={0: 0.68, 1: 0.32}, criterion='entropy',\n",
      "            max_depth=2, max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n"
     ]
    }
   ],
   "source": [
    "# print out the best tuned model of Decision Tree and its accuracy\n",
    "print('Best score: ', classifier.best_score_, '\\nBest Tree: ', classifier.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the GridSearchCV would re-fit on best parameter, it is not neccessary to train my own model\n",
    "dt = classifier.best_estimator_\n",
    "predictions = dt.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8804086538461539\n",
      "F1-Score:  0.8334728033472805\n",
      "Confusion matrix:\n",
      "      0    1\n",
      "0  967  152\n",
      "1   47  498\n"
     ]
    }
   ],
   "source": [
    "# metrics\n",
    "print('Accuracy: ', dt.score(X_test, y_test))\n",
    "print('F1-Score: ', f1_score(y_test, predictions))\n",
    "print('Confusion matrix:\\n', pd.DataFrame(confusion_matrix(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In order to prevent the overfitting problem, doing a 10-folds grid search cross validation on parameter max_depth, also on the criterion and class_weight to find the best parameter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise-deprecating',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'criterion': ('entropy', 'gini'), 'max_depth': [2, 3, 4, 5, 6], 'class_weight': ({0: 0.68, 1: 0.32}, 'balanced', None)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grid search cross validation on parameters \"criterion\" and \"max_depth\"\n",
    "rf = RandomForestClassifier()\n",
    "folds = 10\n",
    "grid_params = {'criterion':('entropy', 'gini'), 'max_depth':[2,3,4,5,6], 'class_weight':({0:0.68, 1:0.32}, 'balanced', None)}\n",
    "classifier = GridSearchCV(rf, grid_params, cv=folds)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score:  0.8969196093163035 \n",
      "Best Tree:  RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "# print out the best tuned model of Random Forest and its accuracy\n",
    "print('Best score: ', classifier.best_score_, '\\nBest Tree: ', classifier.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the GridSearchCV would re-fit on best parameter, it is not neccessary to train my own model\n",
    "rf = classifier.best_estimator_\n",
    "predictions = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8804086538461539\n",
      "F1-Score:  0.8334728033472805\n",
      "Confusion matrix:\n",
      "      0    1\n",
      "0  967  152\n",
      "1   47  498\n"
     ]
    }
   ],
   "source": [
    "# metrics\n",
    "print('Accuracy: ', rf.score(X_test, y_test))\n",
    "print('F1-Score: ', f1_score(y_test, predictions))\n",
    "print('Confusion matrix:\\n', pd.DataFrame(confusion_matrix(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Doing a 10-folds grid search cross validation on parameter learning_rate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise-deprecating',\n",
       "       estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "          learning_rate=1.0, n_estimators=50, random_state=None),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'learning_rate': [0.01, 0.001, 0.0001]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grid search cross validation on parameters learning_rate\n",
    "ab = AdaBoostClassifier()\n",
    "folds = 10\n",
    "grid_params = {'learning_rate':[0.01, 0.001, 0.0001]}\n",
    "classifier = GridSearchCV(ab, grid_params, cv=folds)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score:  0.8898572501878287 \n",
      "Best Tree:  AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=0.01, n_estimators=50, random_state=None)\n"
     ]
    }
   ],
   "source": [
    "# print out the best tuned model of AdaBoost and its accuracy\n",
    "print('Best score: ', classifier.best_score_, '\\nBest Tree: ', classifier.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the GridSearchCV would re-fit on best parameter, it is not neccessary to train my own model\n",
    "ab = classifier.best_estimator_\n",
    "predictions = ab.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8804086538461539\n",
      "F1-Score:  0.8334728033472805\n",
      "Confusion matrix:\n",
      "      0    1\n",
      "0  967  152\n",
      "1   47  498\n"
     ]
    }
   ],
   "source": [
    "# metrics\n",
    "print('Accuracy: ', ab.score(X_test, y_test))\n",
    "print('F1-Score: ', f1_score(y_test, predictions))\n",
    "print('Confusion matrix:\\n', pd.DataFrame(confusion_matrix(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradientBoosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Doing a 10-folds grid search cross validation on parameter learning_rate, max_depth and criterion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise-deprecating',\n",
       "       estimator=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_sampl...      subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "              verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'learning_rate': [0.1, 0.001, 0.0001], 'max_depth': [2, 3, 4, 5, 6], 'criterion': ('friedman_mse', 'mse')},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grid search cross validation on parameters learning_rate and max_depth\n",
    "gb = GradientBoostingClassifier()\n",
    "folds = 10\n",
    "grid_params = {'learning_rate':[0.1, 0.001, 0.0001], 'max_depth':[2,3,4,5,6], 'criterion':('friedman_mse','mse')}\n",
    "classifier = GridSearchCV(gb, grid_params, cv=folds)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score:  0.8969196093163035 \n",
      "Best Tree:  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=2,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "              min_samples_leaf=1, min_samples_split=2,\n",
      "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "              n_iter_no_change=None, presort='auto', random_state=None,\n",
      "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
      "              verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "# print out the best tuned model of AdaBoost and its accuracy\n",
    "print('Best score: ', classifier.best_score_, '\\nBest Tree: ', classifier.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the GridSearchCV would re-fit on best parameter, it is not neccessary to train my own model\n",
    "gb = classifier.best_estimator_\n",
    "predictions = gb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8804086538461539\n",
      "F1-Score:  0.8334728033472805\n",
      "Confusion matrix:\n",
      "      0    1\n",
      "0  967  152\n",
      "1   47  498\n"
     ]
    }
   ],
   "source": [
    "# metrics\n",
    "print('Accuracy: ', gb.score(X_test, y_test))\n",
    "print('F1-Score: ', f1_score(y_test, predictions))\n",
    "print('Confusion matrix:\\n', pd.DataFrame(confusion_matrix(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In order to find the best K for KNN classifier, doing 50 iterations to find out the smallest k with best accuracy.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Splitting the the train data to do the cross validation. The train data and validate data would be 0.7:0.3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spliting the train data into train data and validation data, so that we can use it to find the best k\n",
    "X_train_knn, X_val_knn, y_train_knn, y_val_knn = train_test_split(X_train, y_train, test_size=0.30)\n",
    "\n",
    "results = []\n",
    "for k in range(1, 50):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train_knn, y_train_knn)\n",
    "    results.append([k, knn.score(X_val_knn, y_val_knn)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 0.8312468703054582],\n",
       " [2, 0.800701051577366],\n",
       " [3, 0.8517776664997496],\n",
       " [4, 0.8387581372058087],\n",
       " [5, 0.8422633950926389],\n",
       " [6, 0.8242363545317977],\n",
       " [7, 0.8367551326990486],\n",
       " [8, 0.8182273410115173],\n",
       " [9, 0.827741612418628],\n",
       " [10, 0.8207310966449675],\n",
       " [11, 0.827741612418628],\n",
       " [12, 0.8187280921382073],\n",
       " [13, 0.829243865798698],\n",
       " [14, 0.8182273410115173],\n",
       " [15, 0.8227341011517276],\n",
       " [16, 0.8172258387581373],\n",
       " [17, 0.8307461191787682],\n",
       " [18, 0.8182273410115173],\n",
       " [19, 0.8392588883324987],\n",
       " [20, 0.8192288432648973],\n",
       " [21, 0.8347521281922884],\n",
       " [22, 0.8172258387581373],\n",
       " [23, 0.8302453680520782],\n",
       " [24, 0.8167250876314471],\n",
       " [25, 0.8257386079118678],\n",
       " [26, 0.8177265898848273],\n",
       " [27, 0.827741612418628],\n",
       " [28, 0.8247371056584877],\n",
       " [29, 0.8317476214321482],\n",
       " [30, 0.8192288432648973],\n",
       " [31, 0.8242363545317977],\n",
       " [32, 0.8197295943915874],\n",
       " [33, 0.8267401101652478],\n",
       " [34, 0.8202303455182774],\n",
       " [35, 0.8197295943915874],\n",
       " [36, 0.8127190786179269],\n",
       " [37, 0.8192288432648973],\n",
       " [38, 0.8102153229844767],\n",
       " [39, 0.8147220831246871],\n",
       " [40, 0.801201802704056],\n",
       " [41, 0.8037055583375062],\n",
       " [42, 0.7966950425638458],\n",
       " [43, 0.800200300450676],\n",
       " [44, 0.7976965448172258],\n",
       " [45, 0.7976965448172258],\n",
       " [46, 0.7891837756634953],\n",
       " [47, 0.801201802704056],\n",
       " [48, 0.7886830245368052],\n",
       " [49, 0.7926890335503255]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print out the accuracy for different k\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As the results above, the best accuracy cames with the best k 3, so re-trian the model with k 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-train the model and test with testset\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "predictions = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8335336538461539\n",
      "F1-Score:  0.7593397046046915\n",
      "Confusion matrix:\n",
      "      0    1\n",
      "0  950  169\n",
      "1  108  437\n"
     ]
    }
   ],
   "source": [
    "# metrics\n",
    "print('Accuracy: ', knn.score(X_test, y_test))\n",
    "print('F1-Score: ', f1_score(y_test, predictions))\n",
    "print('Confusion matrix:\\n', pd.DataFrame(confusion_matrix(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Doing a 10-folds grid search cross validation on parameter learning_rate and max_depth**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise-deprecating',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'learning_rate': [0.1, 0.001, 0.0001], 'max_depth': [1, 2, 3, 4, 5, 6]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grid search cross validation on parameters learning_rate and max_depth\n",
    "xgb = XGBClassifier()\n",
    "folds = 10\n",
    "grid_params = {'learning_rate':[0.1, 0.001, 0.0001], 'max_depth':[1,2,3,4,5,6]}\n",
    "classifier = GridSearchCV(xgb, grid_params, cv=folds)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score:  0.8969196093163035 \n",
      "Best Tree:  XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=1, min_child_weight=1, missing=None, n_estimators=100,\n",
      "       n_jobs=1, nthread=None, objective='multi:softprob', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1)\n"
     ]
    }
   ],
   "source": [
    "# print out the best tuned model of AdaBoost and its accuracy\n",
    "print('Best score: ', classifier.best_score_, '\\nBest Tree: ', classifier.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the GridSearchCV would re-fit on best parameter, it is not neccessary to train my own model\n",
    "xgb = classifier.best_estimator_\n",
    "predictions = xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8804086538461539\n",
      "F1-Score:  0.8334728033472805\n",
      "Confusion matrix:\n",
      "      0    1\n",
      "0  967  152\n",
      "1   47  498\n"
     ]
    }
   ],
   "source": [
    "# metrics\n",
    "print('Accuracy: ', xgb.score(X_test, y_test))\n",
    "print('F1-Score: ', f1_score(y_test, predictions))\n",
    "print('Confusion matrix:\\n', pd.DataFrame(confusion_matrix(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training 2 hidden layers neural network with 0.001 learning rate, adam optimizer and logistic sigmoid ativation function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the neural network and test on testset\n",
    "mc = MLPClassifier(hidden_layer_sizes=(200,100,), learning_rate_init=0.001, solver='adam', activation='logistic')\n",
    "mc.fit(X_train, y_train)\n",
    "predictions = mc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8323317307692307\n",
      "F1-Score:  0.7267384916748285\n",
      "Confusion matrix:\n",
      "       0    1\n",
      "0  1014  105\n",
      "1   174  371\n"
     ]
    }
   ],
   "source": [
    "# metrics\n",
    "print('Accuracy: ', mc.score(X_test, y_test))\n",
    "print('F1-Score: ', f1_score(y_test, predictions))\n",
    "print('Confusion matrix:\\n', pd.DataFrame(confusion_matrix(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the Gaussian naive bayes model and test on testset\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train)\n",
    "predictions = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8137019230769231\n",
      "F1-Score:  0.7753623188405797\n",
      "Confusion matrix:\n",
      "      0    1\n",
      "0  819  300\n",
      "1   10  535\n"
     ]
    }
   ],
   "source": [
    "# metrics\n",
    "print('Accuracy: ', nb.score(X_test, y_test))\n",
    "print('F1-Score: ', f1_score(y_test, predictions))\n",
    "print('Confusion matrix:\\n', pd.DataFrame(confusion_matrix(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After training the models above, XGBoost, GradientBoosting, AdaBoost, Random Forest and Decision Tree come out with the best accuracy and best f1 score. Among the models that have best accuray and f1 socre, I would choose the model using decision tree since its more simple and need less time to train than GradientBoosting, Adaboost and XGBoost, also the Simplest one.**\n",
    "```\n",
    "Chosen model: Decision Tree\n",
    "Hyperparameter: criterion='entropy', max_depth=2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>BAC</th>\n",
       "      <th>speeding5MPH</th>\n",
       "      <th>speeding10MPH</th>\n",
       "      <th>Fatality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.003697</td>\n",
       "      <td>-0.001608</td>\n",
       "      <td>-0.019959</td>\n",
       "      <td>-0.024643</td>\n",
       "      <td>0.004651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gender</th>\n",
       "      <td>0.003697</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.122318</td>\n",
       "      <td>-0.014784</td>\n",
       "      <td>-0.005923</td>\n",
       "      <td>0.679881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BAC</th>\n",
       "      <td>-0.001608</td>\n",
       "      <td>0.122318</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.005684</td>\n",
       "      <td>0.012962</td>\n",
       "      <td>-0.136319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speeding5MPH</th>\n",
       "      <td>-0.019959</td>\n",
       "      <td>-0.014784</td>\n",
       "      <td>-0.005684</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.569499</td>\n",
       "      <td>0.028746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speeding10MPH</th>\n",
       "      <td>-0.024643</td>\n",
       "      <td>-0.005923</td>\n",
       "      <td>0.012962</td>\n",
       "      <td>0.569499</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.024826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fatality</th>\n",
       "      <td>0.004651</td>\n",
       "      <td>0.679881</td>\n",
       "      <td>-0.136319</td>\n",
       "      <td>0.028746</td>\n",
       "      <td>0.024826</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Age    Gender       BAC  speeding5MPH  speeding10MPH  \\\n",
       "Age            1.000000  0.003697 -0.001608     -0.019959      -0.024643   \n",
       "Gender         0.003697  1.000000  0.122318     -0.014784      -0.005923   \n",
       "BAC           -0.001608  0.122318  1.000000     -0.005684       0.012962   \n",
       "speeding5MPH  -0.019959 -0.014784 -0.005684      1.000000       0.569499   \n",
       "speeding10MPH -0.024643 -0.005923  0.012962      0.569499       1.000000   \n",
       "Fatality       0.004651  0.679881 -0.136319      0.028746       0.024826   \n",
       "\n",
       "               Fatality  \n",
       "Age            0.004651  \n",
       "Gender         0.679881  \n",
       "BAC           -0.136319  \n",
       "speeding5MPH   0.028746  \n",
       "speeding10MPH  0.024826  \n",
       "Fatality       1.000000  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dui.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As the correlation showing above, Gender and BAC have higher relationship with Fatality**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importances of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.79795034, 0.19929066, 0.        , 0.00275901])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As the importances list above, the second and third features which is Gender and BAC have higher score of importance.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Based on correlation and feature importances above, the top two feature contribute to fatality are Gender and BAC**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Generate Hypotheses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using my selected model Decision tree to generate hypotheses**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data an transform the gender colum\n",
    "dui_test = pd.read_csv('data/dui-test.csv')\n",
    "gender_test = dui_test.iloc[:,1]\n",
    "encoded_column_test = le.transform(gender_test)\n",
    "dui_test['Gender'] = encoded_column_test\n",
    "\n",
    "X_hyps_gen = dui_test.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on the independent variable\n",
    "hyps = dt.predict(X_hyps_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the hyps to test data\n",
    "dui_test['Gender'] = gender_test\n",
    "dui_test['Fatality'] = hyps\n",
    "\n",
    "# write to dui-test-hypotheses.csv\n",
    "dui_test.to_csv(\"dui-test-hypotheses.csv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
